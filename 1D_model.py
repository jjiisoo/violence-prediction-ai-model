# -*- coding: utf-8 -*-
"""최종1_d_ipynb의_사본.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nfQLeRtYNUyu97gR8rrJVqTZMc8480Ls
"""

from google.colab import drive
drive.mount('/content/drive')

# dataset.zip 압축 풀기
!unzip --qq /content/drive/MyDrive/Pre_Dataset -d Dataset

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import librosa
import os

# 데이터 로드 및 전처리
def load_data(data_dir, sr=48000, duration=5):
    X = []
    y = []

    for class_folder in os.listdir(data_dir):
        class_path = os.path.join(data_dir, class_folder)

        if os.path.isdir(class_path):
            for root, _, files in os.walk(class_path):
                for file in files:
                    if file.endswith(".wav"):
                        file_path = os.path.join(root, file)

                        # 음성 파일 로드
                        audio, _ = librosa.load(file_path, sr=sr)

                        # 5초 길이로 자르기 (sr*duration으로 샘플링 길이 계산)
                        target_length = sr * duration
                        if len(audio) < target_length:
                            audio = np.pad(audio, (0, target_length - len(audio)))  # 길이가 부족하면 패딩
                        else:
                            audio = audio[:target_length]  # 길이가 길면 자름

                        X.append(audio)
                        y.append(class_folder)

    return np.array(X), np.array(y)

# 데이터 경로
data_dir = "/content/drive/MyDrive/Pre_Dataset"

# 데이터 로드 (5초 길이로 처리)
X, y = load_data(data_dir, sr=48000, duration=5)

# 레이블 인코딩
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# 데이터 분할 (훈련/테스트)
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

# 데이터 차원 변경 (CNN 입력 형식에 맞게 1D 데이터로 변환)
X_train = X_train[..., np.newaxis]
X_test = X_test[..., np.newaxis]

# CNN 모델 정의
model = models.Sequential([
    layers.Conv1D(64, 3, activation='relu', input_shape=(48000*5, 1)),  # 5초 길이
    layers.MaxPooling1D(2),
    layers.Conv1D(128, 3, activation='relu'),
    layers.MaxPooling1D(2),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(len(np.unique(y_encoded)), activation='softmax')  # 클래스 수에 맞게 출력층 설정
])

# 모델 컴파일
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# 모델 학습
model.fit(X_train, y_train, epochs=1, batch_size=8, validation_data=(X_test, y_test))

# 모델 평가
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc:.4f}")

# torch 관련 라이브러리
from torchvision import models, transforms
from torch.utils.data import DataLoader, Dataset
import librosa
import torch
import torch.nn as nn
import torchaudio
import torch.nn.functional as F
from torch.optim.lr_scheduler import MultiStepLR, StepLR
from torchsummary import summary
from torchvision.datasets import ImageFolder
# 일반 라이브러리
import argparse
import numpy as np
import random
import os
from PIL import Image
import matplotlib.pyplot as plt
import time
from tqdm import tqdm
from sklearn.metrics import f1_score

#스케쥴러
from torch.optim.lr_scheduler import ReduceLROnPlateau

FIXED_LENGTH = 66150

# 모델 학습 관련 파라미터 모음 --> 자유롭게 변경하고, 추가해보세요.

class Args():
  data_type = "1d"
  scheduler = "reducelronplateau"
  model = "resnet"
  n_class = 2
  epoch = 50
  phase = "train"
  model_path = "./model_weight_1d_best.pth"

args = Args()

'''
class TimeStretching(object):
    def __init__(self, rate=0.8):
        self.rate = rate  # 1.0보다 작으면 느려지고, 크면 빨라집니다.

    def __call__(self, y):
        return librosa.effects.time_stretch(y, self.rate)
'''

class PitchShifting(object):
    def __init__(self, n_steps=2):
        self.n_steps = n_steps  # 음정을 얼마나 이동시킬지 (반음 단위)

    def __call__(self, y, sr=22050):
        return librosa.effects.pitch_shift(y, sr, self.n_steps)


class AddNoise(object):
    def __init__(self, noise_factor=0.005):
        self.noise_factor = noise_factor  # 잡음의 크기

    def __call__(self, y):
        noise = np.random.randn(len(y))
        return y + self.noise_factor * noise


class Gain(object):
    def __init__(self, gain_range=(0.9, 1.1)):
        self.gain_range = gain_range  # 볼륨을 랜덤하게 증가시키는 범위

    def __call__(self, y):
        gain = random.uniform(*self.gain_range)
        return y * gain

'''
class TimeShifting(object):
    def __init__(self, shift_max=0.2, shift_direction='both'):
        self.shift_max = shift_max  # 최대 이동 시간 (비율)
        self.shift_direction = shift_direction  # 이동 방향 (앞/뒤/둘 다)

    def __call__(self, y):
        shift = np.random.randint(int(self.shift_max * len(y)))

        if self.shift_direction == 'right':
            shift = -shift  # 오른쪽으로 이동
        elif self.shift_direction == 'both':
            direction = random.choice([-1, 1])  # 방향을 랜덤하게 선택
            shift = shift * direction

        augmented_y = np.roll(y, shift)  # 데이터를 이동시킴
        return augmented_y
'''
# 오디오 데이터셋 클래스에 적용
class AudioDataset(Dataset):
    def __init__(self, directory, transform=None):
        self.directory = directory
        self.transform = transform
        self.filepaths = []
        self.labels = []
        self.class_to_idx = {'non-violence': 0, 'violence': 1}

        for label in range(2):
            class_dir = os.path.join(self.directory, str(label))
            if not os.path.isdir(class_dir):
                continue
            for filename in os.listdir(class_dir):
                if filename.endswith('.wav'):
                    self.filepaths.append(os.path.join(class_dir, filename))
                    self.labels.append(label)

    def __len__(self):
        return len(self.filepaths)

    def __getitem__(self, idx):
        filepath = self.filepaths[idx]
        label = self.labels[idx]

        # 오디오 파일 로드
        y, sr = librosa.load(filepath, sr=22050)

        # 고정된 길이로 패딩 또는 트렁케이션
        if len(y) < FIXED_LENGTH:
            padding = FIXED_LENGTH - len(y)
            y = np.pad(y, (0, padding), 'constant')
        else:
            y = y[:FIXED_LENGTH]

        # 필요에 따라 데이터 전처리/증강 추가
        if self.transform:
            y = self.transform(y)

        return torch.tensor(y, dtype=torch.float32).unsqueeze(0), label


# 증강 기법 적용
augmentations = transforms.Compose([
    #TimeStretching(rate=random.uniform(0.8, 1.2)),
    PitchShifting(n_steps=random.randint(-2, 2)),
    AddNoise(noise_factor=0.005),
    Gain(gain_range=(0.8, 1.2))
    #TimeShifting(shift_max=0.2, shift_direction='both')
])

# 데이터 세트 관련 함수 --> 데이터 증강 기법을 적절하게 추가해보세요.
class TimeMasking(object):
    def __init__(self, T=40, max_masks=1):
        self.T = T
        self.max_masks = max_masks

    def __call__(self, spec):
        for _ in range(0, self.max_masks):
            t = random.randrange(0, self.T)
            t0 = random.randrange(0, spec.shape[1] - t)
            spec[:, t0:t0+t] = 0
        return spec

class FrequencyMasking(object):
    def __init__(self, F=30, max_masks=1):
        self.F = F
        self.max_masks = max_masks

    def __call__(self, spec):
        for _ in range(0, self.max_masks):
            f = random.randrange(0, self.F)
            max_f0 = spec.shape[0] - f
            if max_f0 <= 0:
                continue
            f0 = random.randrange(0, max_f0)
            spec[f0:f0+f, :] = 0
        return spec

class AudioDataset(Dataset):
    def __init__(self, directory, transform=None):
        self.directory = directory
        self.transform = transform
        self.filepaths = []
        self.labels = []
        self.class_to_idx = {'non-violence': 0, 'violence': 1}

        # 폴더 내 모든 레이블을 순회
        for label in range(2):
            class_dir = os.path.join(self.directory, str(label))
            if not os.path.isdir(class_dir):
                continue  # 디렉토리가 존재하지 않을 경우 건너뜀
            for filename in os.listdir(class_dir):
                if filename.endswith('.wav'):
                    self.filepaths.append(os.path.join(class_dir, filename))
                    self.labels.append(label)
        '''
        for label in range(2):
            class_dir = os.path.join(self.directory, str(label))
            for filename in os.listdir(class_dir):

                if filename.endswith('.wav'):
                    self.filepaths.append(os.path.join(class_dir, filename))
                    self.labels.append(label)
                    #self.labels.append(class_name)
        '''

    def __len__(self):
        return len(self.filepaths)

    def __getitem__(self, idx):
        filepath = self.filepaths[idx]
        label = self.labels[idx]

        # 오디오 파일 로드
        y, sr = librosa.load(filepath, sr=22050)  # 원하는 샘플링 주파수로 설정

        # 고정된 길이로 패딩 또는 트렁케이션
        if len(y) < FIXED_LENGTH:
            padding = FIXED_LENGTH - len(y)
            y = np.pad(y, (0, padding), 'constant')
        else:
            y = y[:FIXED_LENGTH]

        # 텐서로 변환
        y = torch.tensor(y, dtype=torch.float32).unsqueeze(0)

        # 필요에 따라 데이터 전처리 추가
        if self.transform:
            y = self.transform(y)

        return y, label

        '''
        #waveform, _ = torchaudio.load(filepath)

        # 웨이브폼 길이 조정
        if waveform.shape[1] < 16000:
            # 길이가 16000보다 짧은 경우, 뒤쪽을 0으로 패딩
            waveform = torch.nn.functional.pad(waveform, (0, 16000 - waveform.shape[1]))
        elif waveform.shape[1] > 16000:
            # 길이가 16000보다 긴 경우, 웨이브폼을 16000으로 자름
            waveform = waveform[:, :16000]

        # 변환 적용
        if self.transform:
            waveform = self.transform(waveform)

        return waveform, label



class ImageDataset(Dataset):
    def __init__(self, directory, transform=None):
        self.directory = directory
        self.transform = transform
        self.classes = sorted(os.listdir(directory))
        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}
        self.samples = []


        for class_name in self.classes:
            class_dir = os.path.join(directory, class_name)
            for image_name in os.listdir(class_dir):
                self.samples.append((os.path.join(class_dir, image_name), self.class_to_idx[class_name]))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        image_path, label = self.samples[idx]
        image = Image.open(image_path).convert('RGB')

        if self.transform:
            image = self.transform(image)

        return image, label
'''

# Residual Block 정의 (1D)
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)
        self.bn1 = nn.BatchNorm1d(out_channels)
        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm1d(out_channels)
        self.downsample = downsample  # 입력 크기를 맞추기 위한 downsample 레이어

    def forward(self, x):
        identity = x
        if self.downsample is not None:
            identity = self.downsample(x)

        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))

        # 입력과 출력 더하기 (Residual 연결)
        out += identity
        out = F.relu(out)
        return out

# ResNet1D 기본 네트워크 정의
class ResNet1D(nn.Module):
    def __init__(self, block, layers, n_class):
        super(ResNet1D, self).__init__()
        self.in_channels = 64

        # 초기 레이어 (1D)
        self.conv1 = nn.Conv1d(1, 64, kernel_size=7, stride=2, padding=3)  # 1채널로 수정
        self.bn1 = nn.BatchNorm1d(64)
        self.pool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)

        # ResNet 레이어 쌓기
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)

        # Adaptive Pooling과 Fully Connected Layer
        self.adaptive_pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Linear(512, n_class)

    def _make_layer(self, block, out_channels, blocks, stride=1):
        downsample = None
        if stride != 1 or self.in_channels != out_channels:
            # 입력과 출력의 크기가 다를 경우 downsample을 사용하여 크기를 맞춤
            downsample = nn.Sequential(
                nn.Conv1d(self.in_channels, out_channels, kernel_size=1, stride=stride),
                nn.BatchNorm1d(out_channels)
            )

        layers = []
        layers.append(block(self.in_channels, out_channels, stride, downsample))
        self.in_channels = out_channels
        for _ in range(1, blocks):
            layers.append(block(out_channels, out_channels))

        return nn.Sequential(*layers)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.pool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.adaptive_pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)

        return x

# ResNet18 구조로 모델 생성 (기본 레이어 블록 수)
def resnet18(n_class=2):
    return ResNet1D(ResidualBlock, [2, 2, 2, 2], n_class)

# 예시: ResNet18 모델 생성
model = resnet18(n_class=2)

from torchinfo import summary

# 1D 모델의 요약 정보 출력
summary(model, input_size=(1, 1, 224), depth=2, col_names=["input_size", "output_size", "num_params"])

# 학습 관련 함수 정의
def train_model(model, train_loader, val_loader, epochs, device, args, patience=30):
    model = model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # 스케줄러 설정
    if args.scheduler == 'multistep':
        scheduler = MultiStepLR(optimizer, milestones=[5, 10], gamma=0.1)
    elif args.scheduler == 'steplr':
        scheduler = StepLR(optimizer, step_size=5, gamma=0.1)
    elif args.scheduler == 'reducelronplateau':
        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)
    else:
        scheduler = None

    # Best model 저장을 위한 초기 설정
    best_val_loss = float('inf')  # 초기에는 큰 값으로 설정하여 이후에 더 작은 값으로 업데이트
    best_model_weights = None
    early_stop_counter = 0  # 얼리스타핑 카운터 초기화

    for epoch in range(epochs):
        # Training
        model.train()
        running_loss = 0.0
        total_batches = len(train_loader)
        correct_predictions = 0
        total_samples = 0
        all_labels = []
        all_predictions = []

        for i, (audio_signals, labels) in enumerate(tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}")):
            audio_signals, labels = audio_signals.to(device), labels.to(device)
            optimizer.zero_grad()

            outputs = model(audio_signals)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            correct_predictions += (predicted == labels).sum().item()
            total_samples += labels.size(0)

            all_labels.extend(labels.cpu().numpy())
            all_predictions.extend(predicted.cpu().numpy())

        avg_loss = running_loss / total_batches
        accuracy = correct_predictions / total_samples
        train_f1_score = f1_score(all_labels, all_predictions, average='weighted')

        print(f"Epoch [{epoch + 1}/{epochs}], Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.4f}, Train F1 Score: {train_f1_score:.4f}")

        # Validation
        model.eval()
        val_loss = 0.0
        correct_val_predictions = 0
        total_val_samples = 0
        all_val_labels = []
        all_val_predictions = []

        with torch.no_grad():
            for audio_signals, labels in tqdm(val_loader, desc="Validation"):
                audio_signals, labels = audio_signals.to(device), labels.to(device)

                outputs = model(audio_signals)
                loss = criterion(outputs, labels)
                val_loss += loss.item()

                _, predicted = torch.max(outputs, 1)
                correct_val_predictions += (predicted == labels).sum().item()
                total_val_samples += labels.size(0)

                all_val_labels.extend(labels.cpu().numpy())
                all_val_predictions.extend(predicted.cpu().numpy())

        avg_val_loss = val_loss / len(val_loader)
        val_accuracy = correct_val_predictions / total_val_samples
        val_f1_score = f1_score(all_val_labels, all_val_predictions, average='weighted')

        print(f"Epoch [{epoch + 1}/{epochs}], Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1 Score: {val_f1_score:.4f}")

        # Best 모델 저장
        if avg_val_loss < best_val_loss:
            print(f"Best model found at epoch {epoch + 1}, saving model...")
            best_val_loss = avg_val_loss
            best_model_weights = model.state_dict()  # Best 모델 가중치 저장
            early_stop_counter = 0  # 성능이 향상되었으므로 카운터 초기화
        else:
            early_stop_counter += 1  # 성능이 개선되지 않으면 카운터 증가
            print(f"No improvement in validation loss for {early_stop_counter} epoch(s).")

        # 얼리스타핑 조건 확인
        if early_stop_counter >= patience:
            print(f"Early stopping triggered after {epoch + 1} epochs.")
            break

        # Scheduler update
        if scheduler is not None:
            if args.scheduler == 'reducelronplateau':
                scheduler.step(avg_val_loss)
            else:
                scheduler.step()

    print('Finished Training')

    # Best 모델 저장
    if best_model_weights is not None:
        torch.save(best_model_weights, f"./model_weight_{args.data_type}_best.pth")
        print("Best model saved.")

    # Best 모델 로드
    model.load_state_dict(best_model_weights)

    return model

pip install torchinfo

from torchinfo import summary

summary(model, input_size=(1, 3, 224, 224), depth=2, col_names=["input_size", "output_size", "num_params"])

from torchsummary import summary
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# 모델 요약
model = model.to(device)

summary(model.to(device), (1, FIXED_LENGTH))

train_dataset = AudioDataset(directory='/content/Dataset/Dataset/train')
val_dataset = AudioDataset(directory='/content/Dataset/Dataset/validation')

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
# Device 설정 --> gpu로 학습하기 위해서 런타임 유형 변경하세요.
device = torch.device("cpu" if torch.cuda.is_available() else "cpu")
print(device)
print(f"학습 데이터 수는 {len(train_dataset)}개 입니다.")
print(f"검증 데이터 수는 {len(val_dataset)}개 입니다.")

# utils에 포함되었던 함수들 + tic toc 추가. tic toc은 수정하지마세요.
def visualize_audio_batch(audio_signals, labels):
    fig, axes = plt.subplots(4, 4, figsize=(10, 5))

    for i, ax in enumerate(axes.flat):
        if i >= 16:  # 16개의 오디오만 표시
            break
        ax.plot(audio_signals[i].t().numpy())  # 오디오 신호 플롯
        ax.set_title(f'Label: {labels[i]}')
        ax.axis('off')

    plt.tight_layout()
    plt.show()

def tic():
    # 현재 시간을 전역 변수에 저장
    global start_time
    start_time = time.time()

def toc():
    # tic()이 호출된 후 경과한 시간을 계산하고 출력
    elapsed_time = time.time() - start_time

    hours = int(elapsed_time // 3600)
    minutes = int((elapsed_time % 3600) // 60)
    seconds = elapsed_time % 60
    print(f"학습에 소요된 시간은 총 : {hours}시간 {minutes}분 {seconds}초 입니다.")

torch.cuda.is_available()

# 학습 시작 및 종료에 걸린 시간을 측정하기 위한 tic - toc
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tic()

model = train_model(model, train_loader, val_loader, epochs=args.epoch, device=device, args=args)

toc()

test_dataset = AudioDataset(directory='/content/Dataset/Dataset/test')
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
print(f"테스트 데이터 수는 {len(test_dataset)}개 입니다.")

def evaluate_model(model, test_loader, device, args):
    if args.model_path:
        model.load_state_dict(torch.load(args.model_path, map_location=device))
    model = model.to(device)

    model.eval()

    criterion = nn.CrossEntropyLoss()

    total = 0
    correct = 0
    total_loss = 0.0
    all_labels = []
    all_predictions = []

    with torch.no_grad():
        for inputs, labels in tqdm(test_loader):
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)
            total_loss += loss.item()

            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            # F1 score 계산을 위한 레이블과 예측 값 저장
            all_labels.extend(labels.cpu().numpy())
            all_predictions.extend(predicted.cpu().numpy())

    avg_loss = total_loss / len(test_loader)
    accuracy = correct / total * 100
    # F1 score 계산, average='macro'
    f1 = f1_score(all_labels, all_predictions, average='macro')

    print(f'Test Accuracy: {accuracy:.2f}%, Avg Loss: {avg_loss:.4f}, F1 Score: {f1:.4f}')

    return accuracy, avg_loss, f1

accuracy, avg_loss, f1 = evaluate_model(model, test_loader, device=device, args=args)
print(f"테스트 데이터의 f1 score는 {f1}")



def evaluate_model(model, test_loader, device, args):
    if args.model_path:
        model.load_state_dict(torch.load(args.model_path, map_location=device))
    model = model.to(device)

    model.eval()

    criterion = nn.CrossEntropyLoss()

    total = 0
    correct = 0
    total_loss = 0.0
    all_labels = []
    all_predictions = []

    with torch.no_grad():
        for inputs, labels in tqdm(test_loader):
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)
            total_loss += loss.item()

            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            # F1 score 계산을 위한 레이블과 예측 값 저장
            all_labels.extend(labels.cpu().numpy())
            all_predictions.extend(predicted.cpu().numpy())

    avg_loss = total_loss / len(test_loader)
    accuracy = correct / total * 100
    # F1 score 계산, average='macro'
    f1 = f1_score(all_labels, all_predictions, average='macro')
    print(f'Test Accuracy: {accuracy:.2f}%, Avg Loss: {avg_loss:.4f}, F1 Score: {f1:.4f}')

    return accuracy, avg_loss, f1